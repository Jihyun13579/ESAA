{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jihyun13579/ESAA/blob/main/0915(%EA%B8%88)%EC%84%B8%EC%85%98_%EB%AA%A8%EB%8D%B8%ED%9B%88%EB%A0%A8_%EC%97%B0%EC%8A%B5%EB%AC%B8%EC%A0%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **| 모델 훈련 연습 문제**\n",
        "___\n",
        "- 출처 : 핸즈온 머신러닝 Ch04 연습문제 1, 5, 9, 10\n",
        "- 개념 문제의 경우 텍스트 셀을 추가하여 정답을 적어주세요."
      ],
      "metadata": {
        "id": "zCu72vDHGMHo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. 수백만 개의 특성을 가진 훈련 세트에서는 어떤 선형 회귀 알고리즘을 사용할 수 있을까요?**\n",
        "___\n",
        "\n",
        "답 : 수백만 개의 특성을 가지고 있는 데이터셋이라면 정규방정식이나 SVD보다는 SGD(but, 편향의 가능성), 미니배치 경사 하강법이 계산 속도에 용이할 것이다."
      ],
      "metadata": {
        "id": "j3g-_Dq9GiuT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. 배치 경사 하강법을 사용하고 에포크마다 검증 오차를 그래프로 나타내봤습니다. 검증 오차가 일정하게 상승되고 있다면 어떤 일이 일어나고 있는 걸까요? 이 문제를 어떻게 해결할 수 있나요?**\n",
        "___\n",
        "\n",
        "답: 검증 오차가 일정하게 상승하고 있다면, 경우에 따라서는 과적합이나, 일정하게 상승한다면 학습률이 너무 커 최적의 하이퍼 파라미터를 지나친 채 더 좋지 않은 파라미터로 발산하고 있는 상황일 수도 있다. 그렇기에, 훈련 세트의 에러 또한 상승하는지 확인하고, 훈련 세트의 에러 또한 증가하고 있다면 학습률을 낮추고 같은 현상이 발생하는지 확인해보는 과정이 필요하다."
      ],
      "metadata": {
        "id": "-pDjW5XcHPOt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. 릿지 회귀를 사용했을 때 훈련 오차가 검증 오차가 거의 비슷하고 둘 다 높았습니다. 이 모델에는 높은 편향이 문제인가요, 아니면 높은 분산이 문제인가요? 규제 하이퍼파라미터 $\\alpha$를 증가시켜야 할까요 아니면 줄여야 할까요?**\n",
        "___\n",
        "검증 오차가 높지만 훈련 오차가 작다면 과적합을 의심해야 하지만, 훈련 오차와 검증 오차가 모두 비슷하게 높다면 모델이 제대로 훈련되지 않은 과소 적합을 의심해야 할 것이다. 편향이 큰 모델은 훈련 데이터에서 과소적합되기 쉬운 특성을 가졌으므로, 이 모델은 높은 편향이 문제일 것이고, 그렇기에 규제 하이퍼파라미터 $\\alpha$를 감소시켜야 할 것이다."
      ],
      "metadata": {
        "id": "nM7JbsLoy7b7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. 다음과 같이 사용해야 하는 이유는?**\n",
        "___\n",
        "- 평범한 선형 회귀(즉, 아무런 규제가 없는 모델) 대신 릿지 회귀\n",
        "- 릿지 회귀 대신 라쏘 회귀\n",
        "- 라쏘 회귀 대신 엘라스틱넷\n",
        "\n",
        "___\n",
        "답:\n",
        "- 규제가 있는 모델이 일반적으로 규제가 없는 모델보다 과적합을 방지하여 성능이 좋기 때문이다.\n",
        "- 라쏘 회귀는 가중치를 완전히 0으로 만드는 경향이 있기 때문에, 변수를 줄여 feature selection을 해주는 모델을 생성하므로 일반적으로 라쏘 회귀가 더 좋을 것이다.\n",
        "- 라쏘 회귀는 특성들이 서로 강하게 연관되어 있을 때 제대로 작동하지 않을 가능성이 있기에, 그런 안정성의 면에서 엘라스틱넷이 더 유용하다."
      ],
      "metadata": {
        "id": "C8tARu-ZzOGx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **추가) 조기 종료를 사용한 배치 경사 하강법으로 소프트맥스 회귀를 구현해보세요(사이킷런은 사용하지 마세요)**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "* 저자 답안 참고\n"
      ],
      "metadata": {
        "id": "QIZpOEYJVIAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(2042)"
      ],
      "metadata": {
        "id": "IHmHzFE8Ro6v"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 데이터 로드하기"
      ],
      "metadata": {
        "id": "5aagZIAdRk0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "list(iris.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwPn9-oVRrOl",
        "outputId": "3dd95138-a264-44ad-dbbe-3de600715147"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data',\n",
              " 'target',\n",
              " 'frame',\n",
              " 'target_names',\n",
              " 'DESCR',\n",
              " 'feature_names',\n",
              " 'filename',\n",
              " 'data_module']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = iris[\"data\"][:, (2, 3)]  # 꽃잎 길이, 꽃잎 넓이\n",
        "y = iris[\"target\"]"
      ],
      "metadata": {
        "id": "pTLD7pu9Rhce"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모든 샘플에 편향 추가\n",
        "X_with_bias = np.c_[np.ones([len(X), 1]), X]"
      ],
      "metadata": {
        "id": "1N7va-KUW6sT"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 데이터 preprocessing"
      ],
      "metadata": {
        "id": "QV4DANe5SVFR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2-1. 데이터 나누는 과정 (train_test_split() 함수 없이)\n",
        "\n"
      ],
      "metadata": {
        "id": "i6m5cAGqXIS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_ratio = 0.2\n",
        "validation_ratio = 0.2\n",
        "total_size = len(X_with_bias)\n",
        "\n",
        "test_size = int(total_size * test_ratio)\n",
        "validation_size = int(total_size * validation_ratio)\n",
        "train_size = total_size - test_size - validation_size\n",
        "\n",
        "rnd_indices = np.random.permutation(total_size)\n",
        "\n",
        "X_train = X_with_bias[rnd_indices[:train_size]]\n",
        "y_train = y[rnd_indices[:train_size]]\n",
        "X_valid = X_with_bias[rnd_indices[train_size:-test_size]]\n",
        "y_valid = y[rnd_indices[train_size:-test_size]]\n",
        "X_test = X_with_bias[rnd_indices[-test_size:]]\n",
        "y_test = y[rnd_indices[-test_size:]]"
      ],
      "metadata": {
        "id": "9-FNu3twSl62"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2-2. 원핫인코딩 함수"
      ],
      "metadata": {
        "id": "YVTmiIVhXDST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_one_hot(y):\n",
        "    n_classes = y.max() + 1\n",
        "    m = len(y)\n",
        "    Y_one_hot = np.zeros((m, n_classes))\n",
        "    Y_one_hot[np.arange(m), y] = 1\n",
        "    return Y_one_hot"
      ],
      "metadata": {
        "id": "HffDC4miXDB-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 작동 확인\n",
        "to_one_hot(y_train[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSSwrynOXMcj",
        "outputId": "613ba3e4-e35c-4051-a0f1-ecfb8668494a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2-3. 훈련 데이터셋, 테스트 데이터셋 타깃 클래스 확률 담은 행렬 생성하기"
      ],
      "metadata": {
        "id": "GhvoFgiIXhLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train_one_hot = to_one_hot(y_train)\n",
        "Y_valid_one_hot = to_one_hot(y_valid)\n",
        "Y_test_one_hot = to_one_hot(y_test)"
      ],
      "metadata": {
        "id": "2F6yNvAYXml5"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. 소프트맥스 함수 생성하기"
      ],
      "metadata": {
        "id": "cla7yrKtSA5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(logits):\n",
        "    exps = np.exp(logits)\n",
        "    exp_sums = np.sum(exps, axis=1, keepdims=True)\n",
        "    return exps / exp_sums"
      ],
      "metadata": {
        "id": "aPR_aqAnSBOt"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. 입력과 출력의 개수 정의하기"
      ],
      "metadata": {
        "id": "qmhwWnZGXonK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_inputs = X_train.shape[1] # == 3 (특성 2개와 편향)\n",
        "n_outputs = len(np.unique(y_train))   # == 3 (3개의 붓꽃 클래스)"
      ],
      "metadata": {
        "id": "Nn-UDXUqXpJI"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. 비용함수 구현하여 소프트맥스 모델 훈련시키기"
      ],
      "metadata": {
        "id": "vIHFQ9KgXvWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eta = 0.01\n",
        "n_iterations = 5001\n",
        "m = len(X_train)\n",
        "epsilon = 1e-7\n",
        "\n",
        "Theta = np.random.randn(n_inputs, n_outputs)\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    logits = X_train.dot(Theta)\n",
        "    Y_proba = softmax(logits)\n",
        "    if iteration % 500 == 0:\n",
        "        loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
        "        print(iteration, loss)\n",
        "    error = Y_proba - Y_train_one_hot\n",
        "    gradients = 1/m * X_train.T.dot(error)\n",
        "    Theta = Theta - eta * gradients"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87vwHNl9WFuh",
        "outputId": "ac24c2ac-cf02-49c7-ad70-9384585dd060"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 2.0098317606794267\n",
            "500 0.7775485994012685\n",
            "1000 0.6358595299644286\n",
            "1500 0.553854152323856\n",
            "2000 0.5002301566083223\n",
            "2500 0.46167353758270924\n",
            "3000 0.43206307086901297\n",
            "3500 0.4082683127711592\n",
            "4000 0.3885249426733036\n",
            "4500 0.3717545031055311\n",
            "5000 0.35725297209557083\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5-2. 훈련시킨 모델 파라미터 확인하기"
      ],
      "metadata": {
        "id": "ho6bChSyX5m5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Theta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bceVJETQX_Sj",
        "outputId": "8773a604-9fd0-41ee-a1bf-c3ac65423a61"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3.30012252, -0.19360882, -3.26721327],\n",
              "       [-1.20747709, -0.58488635, -0.4152438 ],\n",
              "       [-2.1970059 ,  0.25677352,  1.72547638]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5-3. 검증 데이터셋에 대한 예측 정확도 확인하기"
      ],
      "metadata": {
        "id": "M9Stg8cqYBPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits = X_valid.dot(Theta)\n",
        "Y_proba = softmax(logits)\n",
        "y_predict = np.argmax(Y_proba, axis=1)\n",
        "\n",
        "accuracy_score = np.mean(y_predict == y_valid)\n",
        "accuracy_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D3iPyxFYGSz",
        "outputId": "81e69411-4521-4ad4-a91e-b9ff9f0b518f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8666666666666667"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. L2 규제 추가한 모델 생성하기"
      ],
      "metadata": {
        "id": "5CUnhKmVYLJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eta = 0.1\n",
        "n_iterations = 5001\n",
        "m = len(X_train)\n",
        "epsilon = 1e-7\n",
        "alpha = 0.1  # 규제 하이퍼파라미터\n",
        "\n",
        "Theta = np.random.randn(n_inputs, n_outputs)\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    logits = X_train.dot(Theta)\n",
        "    Y_proba = softmax(logits)\n",
        "    if iteration % 500 == 0:\n",
        "        xentropy_loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
        "        l2_loss = 1/2 * np.sum(np.square(Theta[1:]))\n",
        "        loss = xentropy_loss + alpha * l2_loss\n",
        "        print(iteration, loss)\n",
        "    error = Y_proba - Y_train_one_hot\n",
        "    gradients = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]\n",
        "    Theta = Theta - eta * gradients"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSeJz9R7YPSH",
        "outputId": "54638fd7-731c-4c4f-e145-dcbf85f39895"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 2.968736899582155\n",
            "500 0.5197864820932234\n",
            "1000 0.4910558310637946\n",
            "1500 0.48254537680629156\n",
            "2000 0.4793184131301125\n",
            "2500 0.47798748905462335\n",
            "3000 0.4774151434812393\n",
            "3500 0.4771630781432382\n",
            "4000 0.4770504425998878\n",
            "4500 0.47699964640820014\n",
            "5000 0.47697660148778775\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 정확도 측정하기\n",
        "logits = X_valid.dot(Theta)\n",
        "Y_proba = softmax(logits)\n",
        "y_predict = np.argmax(Y_proba, axis=1)\n",
        "\n",
        "accuracy_score = np.mean(y_predict == y_valid)\n",
        "accuracy_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPSDWMKZYRjB",
        "outputId": "a026c6dc-ffa6-4045-ec21-fa4c5a50fd58"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9333333333333333"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. 함수에 조기 종료 추가해보기"
      ],
      "metadata": {
        "id": "3SuIvUEkYV38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eta = 0.1\n",
        "n_iterations = 200001\n",
        "m = len(X_train)\n",
        "epsilon = 1e-7\n",
        "alpha = 0.1  # 규제 하이퍼파라미터\n",
        "best_loss = np.infty\n",
        "\n",
        "Theta = np.random.randn(n_inputs, n_outputs)\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    logits = X_train.dot(Theta)\n",
        "    Y_proba = softmax(logits)\n",
        "    error = Y_proba - Y_train_one_hot\n",
        "    gradients = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]\n",
        "    Theta = Theta - eta * gradients\n",
        "\n",
        "    logits = X_valid.dot(Theta)\n",
        "    Y_proba = softmax(logits)\n",
        "    xentropy_loss = -np.mean(np.sum(Y_valid_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
        "    l2_loss = 1/2 * np.sum(np.square(Theta[1:]))\n",
        "    loss = xentropy_loss + alpha * l2_loss\n",
        "    if iteration % 500 == 0:\n",
        "        print(iteration, loss)\n",
        "    if loss < best_loss:\n",
        "        best_loss = loss\n",
        "    else:\n",
        "        print(iteration - 1, best_loss)\n",
        "        print(iteration, loss, \"조기 종료!\")\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Mu4mBO8YUyw",
        "outputId": "05f231ae-ed9e-45f9-bd10-1aa135db067e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 2.458575990910144\n",
            "500 0.6254423728798993\n",
            "1000 0.5882142845109087\n",
            "1500 0.5763441453349325\n",
            "2000 0.5715138720647914\n",
            "2500 0.5693183708682336\n",
            "3000 0.5682415558806976\n",
            "3500 0.5676796835958234\n",
            "4000 0.5673703526241188\n",
            "4500 0.5671919301254713\n",
            "5000 0.5670848797599896\n",
            "5500 0.5670185618550828\n",
            "6000 0.5669764386370089\n",
            "6500 0.5669491753075212\n",
            "7000 0.5669312853598879\n",
            "7500 0.5669194300709608\n",
            "8000 0.566911519229655\n",
            "8500 0.5669062149620635\n",
            "9000 0.5669026465764967\n",
            "9500 0.5669002405051871\n",
            "10000 0.566898615622627\n",
            "10500 0.5668975171312067\n",
            "11000 0.5668967739664758\n",
            "11500 0.5668962709448538\n",
            "12000 0.5668959303540688\n",
            "12500 0.5668956996914155\n",
            "13000 0.5668955434528538\n",
            "13500 0.5668954376142012\n",
            "14000 0.5668953659122526\n",
            "14500 0.5668953173343964\n",
            "15000 0.5668952844219793\n",
            "15500 0.5668952621227049\n",
            "16000 0.5668952470139689\n",
            "16500 0.5668952367770348\n",
            "17000 0.5668952298409461\n",
            "17500 0.5668952251413407\n",
            "18000 0.5668952219570736\n",
            "18500 0.5668952197995353\n",
            "19000 0.5668952183376672\n",
            "19500 0.5668952173471589\n",
            "20000 0.5668952166760258\n",
            "20500 0.5668952162212901\n",
            "21000 0.5668952159131772\n",
            "21500 0.5668952157044107\n",
            "22000 0.566895215562958\n",
            "22500 0.5668952154671145\n",
            "23000 0.5668952154021745\n",
            "23500 0.5668952153581732\n",
            "24000 0.5668952153283597\n",
            "24500 0.566895215308159\n",
            "25000 0.5668952152944716\n",
            "25500 0.5668952152851977\n",
            "26000 0.5668952152789138\n",
            "26500 0.5668952152746564\n",
            "27000 0.5668952152717714\n",
            "27500 0.5668952152698168\n",
            "28000 0.5668952152684925\n",
            "28500 0.566895215267595\n",
            "29000 0.5668952152669869\n",
            "29500 0.5668952152665749\n",
            "30000 0.5668952152662958\n",
            "30500 0.5668952152661068\n",
            "30585 0.5668952152660811\n",
            "30586 0.5668952152660811 조기 종료!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 정확도 측정해보기\n",
        "logits = X_valid.dot(Theta)\n",
        "Y_proba = softmax(logits)\n",
        "y_predict = np.argmax(Y_proba, axis=1)\n",
        "\n",
        "accuracy_score = np.mean(y_predict == y_valid)\n",
        "accuracy_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRF7z2QsYiVn",
        "outputId": "2bfcffec-8026-4c54-d4d9-879d0c0a7bc4"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9333333333333333"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. 테스스셋에 대한 모델 최종 정확도 예측하기"
      ],
      "metadata": {
        "id": "1C0n5ySEYuZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits = X_test.dot(Theta)\n",
        "Y_proba = softmax(logits)\n",
        "y_predict = np.argmax(Y_proba, axis=1)\n",
        "\n",
        "accuracy_score = np.mean(y_predict == y_test)\n",
        "accuracy_score\n",
        "# 결과 : 매우 높음!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKUq0DBgYxkx",
        "outputId": "faef7730-f156-465b-db63-53430629cfa7"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    }
  ]
}